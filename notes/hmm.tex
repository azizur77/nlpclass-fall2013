\documentclass[11pt,letterpaper]{article}
\topmargin -.5truein
\textheight 9.0truein
\oddsidemargin 0truein
\evensidemargin 0truein
\textwidth 6.5truein
\setlength{\parskip}{5pt}
\setlength\parindent{0pt}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{caption}
\usepackage{subcaption}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{qtree}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage[all]{xy}
\usetikzlibrary{arrows,snakes,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}

\newcommand{\bs}{\textbackslash}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\ngramstart}{\ensuremath{\langle S \rangle}}
\newcommand{\ngramend}{\ensuremath{\langle E \rangle}}
\newcommand{\ngramunk}{\ensuremath{\langle unk \rangle}}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\title{NLP: Hidden Markov Models}
\author{Dan Garrette\\\small{dhg@cs.utexas.edu}}

\begin{document}
\maketitle



\section{Tagging}

\begin{itemize}
  \item Named entities
  \item Parts of speech
\end{itemize}




\section{Parts of Speech}

Tagsets

\begin{itemize}
  \item Google Universal Tagset, 12: Noun, Verb, Adjective, Adverb, Pronoun, Determiner, Adposition (prepositions and postpositions), Numerals, Conjunctions, Particles, Punctuation, Other
  \item Penn Treebank, 45.  Includes 4 categores of noun, 4 categories of pronoun, and 6 categories of verb.
  \item Brown Corpus, 87
\end{itemize}

Deciding what categories

\begin{itemize}
  \item Auxilliary verbs?  ``I \textit{had} gone''
  \item Numbers as adjectives? ``I saw 4 cars''
  \item Count vs. mass nouns?  ``some apple'' vs. ``two apples'', ``some snow'' vs. ``two snows''
\end{itemize}

Uses

\begin{itemize}
  \item Parsing: determiner and noun should connect first, then to verb
  \item Speech synthesis: OBject (noun) vs. obJECT (verb), CONtent (noun) vs. conTENT (adj)
\end{itemize}

Rule-based

\begin{itemize}
  \item ``if it ends in `-tion' '' $\rightarrow$ Noun
  \item ``if it ends in `-ize' '' $\rightarrow$ Verb
  \item ``if it starts with `re-' '' $\rightarrow$ Verb
  \item ``if it starts with a capital letter'' $\rightarrow$ Proper Noun
  \item ``if it's preceded by `the' '' $\rightarrow$ Noun
  \item ``if it's followed by \textit{'s} '' $\rightarrow$ Noun
  \item ``if it's preceded by an adjective'' $\rightarrow$ Noun
  \item Or just memorize a big list of words and tags?
    \begin{itemize}
      \item Ambiguity?
      \item Picking the most common tag for a word $\rightarrow$ 90\% accuracy (thought state-of-the-art is 90\%)
    \end{itemize}
\end{itemize}

Open vs. Closed

\begin{itemize}
  \item Open class tags: new words are frequently added
    \begin{itemize}
      \item noun, verb, adjective, adverb
      \item ``Twitter'', to ``tweet'', ``twitterish''
    \end{itemize}
  \item Closed class tags: new words are rarely added
    \begin{itemize}
      \item determiner, preposition, pronouns
      \item Don't want to completely close off new words
      \item Maybe \textit{alongside} (preposition) wasn't seen in training, only test
      \item New domains: ``u'' as a pronoun
    \end{itemize}
\end{itemize}

Complexitites:

\begin{itemize}
  \item Ambiguity:
    \begin{itemize}
      \item ``buy a book'' (noun) vs. ``book a flight'' (verb)
      \item ``talk over the deal'' (particle) vs. ``talk over the phone'' (prep)
    \end{itemize}
  \item Phrasal verbs: ``turn down'', ``rule out''
    \begin{itemize}
      \item ``he went on for days''
      \item ``he went on a boat''
    \end{itemize}
\end{itemize}



\section{Hidden Markov Models}


\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 13mm, thick, draw =black!80, node distance = 20mm]
\tikzstyle{obsv}=[main, fill = black!10]
\tikzstyle{hidden}=[node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]

  \node[main] (t1)                {$t_1$};
  \node[obsv] (w1) [below of =t1] {$w_1$};
  \node[main] (t2) [right of =t1] {$t_2$};
  \node[obsv] (w2) [below of =t2] {$w_2$};
  \node[main] (t3) [right of =t2] {$t_3$};
  \node[obsv] (w3) [below of =t3] {$w_3$};
  \node[main] (t4) [right of =t3] {$t_4$};
  \node[obsv] (w4) [below of =t4] {$w_4$};
  \node[hidden] (tI1) [right of =t4, xshift=-5mm] {$\dots$};
  \node[hidden] (wI1) [below of =tI1] {$\dots$};
  \node[hidden] (tI2) [right of =tI1, xshift=-8mm] {$\dots$};
  \node[hidden] (wI2) [below of =tI2] {$\dots$};
  \node[main] (tT) [right of=tI2, xshift=-1mm] {$w_T$};
  \node[obsv] (wT) [below of=tT] {$w_T$};

  \path (t2) edge [connect] (t1)
        (t3) edge [connect] (t2)
        (t4) edge [connect] (t3)
        (tT) edge [connect] (tI2)
        (t1) edge [connect] (w1)
        (t2) edge [connect] (w2)
        (t3) edge [connect] (w3)
        (t4) edge [connect] (w4)
        (tT) edge [connect] (wT);

\end{tikzpicture}
\caption{Second-order HMM showing conditional dependencies}
\end{figure} 

\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 13mm, thick, draw =black!80, node distance = 20mm]
\tikzstyle{obsv}=[main, fill = black!10]
\tikzstyle{hidden}=[node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]

  \node[main] (t1)                {\ngramstart};
  \node[obsv] (w1) [below of =t1] {\ngramstart};
  \node[main] (t2) [right of =t1] {D};
  \node[obsv] (w2) [below of =t2] {the};
  \node[main] (t3) [right of =t2] {N};
  \node[obsv] (w3) [below of =t3] {man};
  \node[main] (t4) [right of =t3] {V};
  \node[obsv] (w4) [below of =t4] {walked};
  \node[main] (t7) [right of =t4] {.};
  \node[obsv] (w7) [below of =t7] {.};
  \node[main] (tT) [right of =t7, xshift=-1mm] {\ngramend};
  \node[obsv] (wT) [below of =tT] {\ngramend};

  \path (t2) edge [connect] (t1)
        (t3) edge [connect] (t2)
        (t4) edge [connect] (t3)
        (t7) edge [connect] (t4)
        (tT) edge [connect] (t7)

        (t1) edge [connect] (w1)
        (t2) edge [connect] (w2)
        (t3) edge [connect] (w3)
        (t4) edge [connect] (w4)
        (t7) edge [connect] (w7)
        (tT) edge [connect] (wT);

\end{tikzpicture}
\caption{Second-order HMM for the sentence ``the man walks .''}
\end{figure} 



Similar to N-Gram models

\begin{itemize}
  \item Model the text as a \textit{sequence}
    \begin{itemize}
      \item Bad assumption, but less sparse
    \end{itemize}
  \item For ngrams, we modeled the probability of each word conditioned on the previous n-1 words.
  \item Here, we model each \textit{tag} conditioned on previous \textit{tags}
  \item Still uses Markov assumption: only look back a few tags
    \begin{itemize}
      \item Again, bad assumption, but less sparse
    \end{itemize}
  \item Note that we have to condition words on tags because otherwise 
\end{itemize}

Derivation

\begin{itemize}
  \item We want the most likely tag sequence for a sequence of words: 
        $p(t_1, t_2, ..., t_n \mid w_1, w_2, ..., w_n)$
  \item For simplicity, we'll write this as $p(t_1^n \mid w_1^n)$
  \item So we want  $\hat{t}_1^n = \text{argmax}_{t_1^n}~p(t_1^n \mid w_1^n)$, but it's hard to estimate
  \item Bayes rule: $\hat{t}_1^n = \text{argmax}_{t_1^n}~\frac{p(w_1^n \mid t_1^n) \cdot p(t_1^n)}{p(w_1^n)}
                                 = \text{argmax}_{t_1^n}~p(w_1^n \mid t_1^n) \cdot p(t_1^n)$
  \item Two major independence assumptions:
    \begin{itemize}
      \item Like ngrams, assume probability of a sequence is recent past: \vspace{2mm} \\
            $p(t_1^n) \approx p(t_2 \mid t_1) \cdot 
                              p(t_2 \mid t_2) \cdot 
                              ...  \cdot 
                              p(t_n \mid t_{n-1})
                       = \prod_{i=1}^n p(t_i \mid t_{i-1})$
      \item Also assume word is only dependent on its tag: \vspace{2mm} \\
            $p(w_1^n \mid t_1^n) \approx p(w_1 \mid t_n) \cdot 
                                         p(w_2 \mid t_n) \cdot 
                                         ...  \cdot 
                                         p(w_n \mid t_n)
                                 = \prod_{i=1}^n p(w_i \mid t_i)$
      \item Together: 
        \begin{align*}
        \hat{t}_1^n &= \text{argmax}_{t_1^n}~p(t_1^n \mid w_1^n) \\
                    &= \text{argmax}_{t_1^n}~p(w_1^n \mid t_1^n) \cdot p(t_1^n) \\
                    &\approx \text{argmax}_{t_1^n}~\prod_{i=1}^n p(w_i \mid t_i) \cdot p(t_i \mid t_{i-1})
        \end{align*}
    \end{itemize}
\end{itemize}







Example dataset:
\vspace{-2mm}
\begin{verbatim}
    <S>|<S> the|D dog|N runs|V   .|.   <E>|<E> 
    <S>|<S> the|D dog|N walks|V  .|.   <E>|<E> 
    <S>|<S> the|D man|N walks|V  .|.   <E>|<E> 
    <S>|<S> a|D   man|N walks|V  the|D dog|N   .|. <E>|<E> 
    <S>|<S> the|D cat|N walks|V  .|.   <E>|<E> 
    <S>|<S> the|D dog|N chases|V the|D cat|N   .|. <E>|<E> 
\end{verbatim}



\newsavebox{\starttable}
\sbox{\starttable}{
    \begin{tabular}{l l}
      \hline
      \multicolumn{2}{|c|}{{\cellcolor{gray!40}\ngramstart}} \tabularnewline
      \hline
      \ngramstart & 1.0
    \end{tabular}
}

\newsavebox{\Dtable}
\sbox{\Dtable}{
    \begin{tabular}{l l}
      \hline
      \multicolumn{2}{|c|}{{\cellcolor{gray!40}D}} \tabularnewline
      \hline
      the & 0.87 \tabularnewline
      a   & 0.13
    \end{tabular}
}

\newsavebox{\Ntable}
\sbox{\Ntable}{
    \begin{tabular}{l l}
      \hline
      \multicolumn{2}{|c|}{{\cellcolor{gray!40}N}} \tabularnewline
      \hline
      man & 0.25 \tabularnewline
      dog & 0.50 \tabularnewline
      cat & 0.25
    \end{tabular}
}

\newsavebox{\Vtable}
\sbox{\Vtable}{
    \begin{tabular}{l l}
      \hline
      \multicolumn{2}{|c|}{{\cellcolor{gray!40}V}} \tabularnewline
      \hline
      walks  & 0.66 \tabularnewline
      runs   & 0.17 \tabularnewline
      chases & 0.17
    \end{tabular}
}

\newsavebox{\stoptable}
\sbox{\stoptable}{
    \begin{tabular}{l l}
      \hline
      \multicolumn{2}{|c|}{{\cellcolor{gray!40}.}} \tabularnewline
      \hline
      . & 1.0
    \end{tabular}
}

\newsavebox{\ngramendtable}
\sbox{\ngramendtable}{
    \begin{tabular}{l l}
      \hline
      \multicolumn{2}{|c|}{{\cellcolor{gray!40}\ngramend}} \tabularnewline
      \hline
      \ngramend & 1.0
    \end{tabular}
}


\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{main}=[rectangle, thick, draw =black!80, node distance = 40mm]
\tikzstyle{obsv}=[main, fill = black!10]
\tikzstyle{hidden}=[node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]

  \node[main] (start) []           {\usebox{\starttable}};
  \node[main] (D)     [right of =start]          {\usebox{\Dtable}};
  \node[main] (N)     [right of =D, yshift= 70]  {\usebox{\Ntable}};
  \node[main] (V)     [right of =D, yshift=-70]  {\usebox{\Vtable}};
  \node[main] (stop)  [right of =D, xshift=40mm] {\usebox{\stoptable}};
  \node[main] (end)   [right of =stop]           {\usebox{\ngramendtable}};


  \path (start) edge [connect] node [pos=0.5, above, blue, sloped] {1.0} (D);
  \path (D) edge [connect] node [pos=0.5, above, blue, sloped] {1.0} (N);
  \path (N) edge [connect] node [pos=0.5, above, blue, sloped] {0.80} (V);
  \path (N) edge [connect] node [pos=0.5, above, blue, sloped] {0.20} (stop);
  \path (V) edge [connect, red] node [pos=0.5, above, blue, sloped] {0.33} (D);
  \path (V) edge [connect] node [pos=0.5, above, blue, sloped] {0.67} (stop);
  \path (stop) edge [connect] node [pos=0.5, above, blue, sloped] {1.0} (end);
  

\end{tikzpicture}
\caption{Finite state machine.  Missing arrows are assumed to be zero probabilities.  With smoothing, there would be an arrow in \textit{both directions} between \textit{every} pair of words.}
\end{figure} 






\end{document}

